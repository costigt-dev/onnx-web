<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://onnx-web.ai/docs/setup-guide/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Setup Guide - onnx-web docs</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Setup Guide";
        var mkdocs_page_input_path = "setup-guide.md";
        var mkdocs_page_url = "/docs/setup-guide/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> onnx-web docs
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">onnx-web</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../api/">API</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../chain-pipelines/">Chain Pipelines</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../compatibility/">Compatibility</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../converting-models/">Converting Models</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dev-test/">Development and Testing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting-started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../runpod-readme/">ONNX Web</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../server-admin/">Server Administration</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Setup Guide</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#contents">Contents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cross-platform-method">Cross-platform method</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#install-git-and-python">Install Git and Python</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#note-about-setup-paths">Note about setup paths</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#create-a-virtual-environment">Create a virtual environment</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#install-pip-packages">Install pip packages</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#for-amd-on-linux-pytorch-rocm-and-onnx-runtime-rocm">For AMD on Linux: PyTorch ROCm and ONNX runtime ROCm</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#for-amd-on-windows-pytorch-cpu-and-onnx-runtime-directml">For AMD on Windows: PyTorch CPU and ONNX runtime DirectML</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#for-cpu-everywhere-pytorch-cpu-and-onnx-runtime-cpu">For CPU everywhere: PyTorch CPU and ONNX runtime CPU</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#for-nvidia-everywhere-install-pytorch-gpu-and-onnx-gpu">For Nvidia everywhere: Install PyTorch GPU and ONNX GPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#test-the-models">Test the models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#download-the-web-ui-bundle">Download the web UI bundle</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#windows-specific-methods">Windows-specific methods</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#windows-all-in-one-bundle">Windows all-in-one bundle</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#windows-python-installer">Windows Python installer</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../user-guide/">User Guide</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">onnx-web docs</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Setup Guide</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/ssube/onnx-web/edit/master/docs/setup-guide.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="setup-guide">Setup Guide</h1>
<p>This guide covers the setup process for onnx-web, including downloading the Windows bundle.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#setup-guide">Setup Guide</a><ul>
<li><a href="#contents">Contents</a></li>
<li><a href="#cross-platform-method">Cross-platform method</a><ul>
<li><a href="#install-git-and-python">Install Git and Python</a></li>
<li><a href="#note-about-setup-paths">Note about setup paths</a></li>
<li><a href="#create-a-virtual-environment">Create a virtual environment</a></li>
<li><a href="#install-pip-packages">Install pip packages</a><ul>
<li><a href="#for-amd-on-linux-pytorch-rocm-and-onnx-runtime-rocm">For AMD on Linux: PyTorch ROCm and ONNX runtime ROCm</a></li>
<li><a href="#for-amd-on-windows-pytorch-cpu-and-onnx-runtime-directml">For AMD on Windows: PyTorch CPU and ONNX runtime DirectML</a></li>
<li><a href="#for-cpu-everywhere-pytorch-cpu-and-onnx-runtime-cpu">For CPU everywhere: PyTorch CPU and ONNX runtime CPU</a></li>
<li><a href="#for-nvidia-everywhere-install-pytorch-gpu-and-onnx-gpu">For Nvidia everywhere: Install PyTorch GPU and ONNX GPU</a></li>
</ul>
</li>
<li><a href="#test-the-models">Test the models</a></li>
<li><a href="#download-the-web-ui-bundle">Download the web UI bundle</a></li>
</ul>
</li>
<li><a href="#windows-specific-methods">Windows-specific methods</a><ul>
<li><a href="#windows-all-in-one-bundle">Windows all-in-one bundle</a></li>
<li><a href="#windows-python-installer">Windows Python installer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="cross-platform-method">Cross-platform method</h2>
<p>This works on both Linux and Windows, for both AMD and Nvidia, but requires some familiarity with the command line.</p>
<h3 id="install-git-and-python">Install Git and Python</h3>
<p>Install Git and Python 3.10 for your environment:</p>
<ul>
<li>https://gitforwindows.org/</li>
<li>https://www.python.org/downloads/</li>
</ul>
<p>The latest version of git should be fine. Python should be 3.9 or 3.10, although 3.8 and 3.11 may work if the correct
packages are available for your platform. If you already have Python installed for another form of Stable Diffusion,
that should work, but make sure to verify the version in the next step.</p>
<p>Make sure you have Python 3.9 or 3.10:</p>
<pre><code class="language-shell">&gt; python --version
Python 3.10
</code></pre>
<p>If your system differentiates between Python 2 and 3 and uses the <code>python3</code> and <code>pip3</code> commands for the Python 3.x
tools, make sure to adjust the commands shown here. They should otherwise be the same: <code>python3 --version</code>.</p>
<p>Once you have those basic packages installed, clone this git repository:</p>
<pre><code class="language-shell">&gt; git clone https://github.com/ssube/onnx-web.git
</code></pre>
<h3 id="note-about-setup-paths">Note about setup paths</h3>
<p>This project contains both Javascript and Python, for the client and server respectively. Make sure you are in the
correct directory when working with each part.</p>
<p>Most of these setup commands should be run in the Python environment and the <code>api/</code> directory:</p>
<pre><code class="language-shell">&gt; cd api
&gt; pwd
/home/ssube/code/github/ssube/onnx-web/api
</code></pre>
<p>The Python virtual environment will be created within the <code>api/</code> directory.</p>
<p>The Javascript client can be built and run within the <code>gui/</code> directory.</p>
<h3 id="create-a-virtual-environment">Create a virtual environment</h3>
<p>Change into the <code>api/</code> directory, then create a virtual environment:</p>
<pre><code class="language-shell">&gt; pip install virtualenv
&gt; python -m venv onnx_env
</code></pre>
<p>This will contain all of the pip libraries. If you update or reinstall Python, you will need to recreate the virtual
environment.</p>
<p>If you receive an error like <code>Error: name 'cmd' is not defined</code>, there may be <a href="https://www.mail-archive.com/debian-bugs-dist@lists.debian.org/msg1884072.html">a bug in the <code>venv</code> module</a> on certain
Debian-based systems. You may need to install venv through apt instead:</p>
<pre><code class="language-shell">&gt; sudo apt install python3-venv   # only if you get an error
</code></pre>
<p>Every time you start using ONNX web, activate the virtual environment:</p>
<pre><code class="language-shell"># on linux:
&gt; source ./onnx_env/bin/activate

# on windows:
&gt; .\onnx_env\Scripts\Activate.bat
</code></pre>
<p>Update pip itself:</p>
<pre><code class="language-shell">&gt; python -m pip install --upgrade pip
</code></pre>
<h3 id="install-pip-packages">Install pip packages</h3>
<p>You can install all of the necessary packages at once using <a href="./api/requirements/base.txt">the <code>requirements/base.txt</code> file</a>
and the <code>requirements/</code> file for your platform. Install them in separate commands and make sure to install the
platform-specific packages first:</p>
<pre><code class="language-shell">&gt; pip install -r requirements/amd-linux.txt
&gt; pip install -r requirements/base.txt
# or
&gt; pip install -r requirements/amd-windows.txt
&gt; pip install -r requirements/base.txt
# or
&gt; pip install -r requirements/cpu.txt
&gt; pip install -r requirements/base.txt
# or
&gt; pip install -r requirements/nvidia.txt
&gt; pip install -r requirements/base.txt
</code></pre>
<p>Only install one of the platform-specific requirements files, otherwise you may end up with the wrong version of
PyTorch or the ONNX runtime. The full list of available ONNX runtime packages <a href="https://download.onnxruntime.ai/">can be found here
</a>.</p>
<p>If you have successfully installed both of the requirements files for your platform, you do not need to install
any of the packages shown in the following sections and you should <a href="#test-the-models">skip directly to testing the models</a>.</p>
<p>The ONNX runtime nightly packages used by the <code>requirements/*-nightly.txt</code> files can be substantially faster than the
last release, but may not always be stable. Many of the nightly packages are specific to one version of Python and
some are only available for Python 3.8 and 3.9, so you may need to find the correct package for your environment. If
you are using Python 3.10, download the <code>cp310</code> package. For Python 3.9, download the <code>cp39</code> package, and so on.
Installing with pip will figure out the correct package for you.</p>
<h4 id="for-amd-on-linux-pytorch-rocm-and-onnx-runtime-rocm">For AMD on Linux: PyTorch ROCm and ONNX runtime ROCm</h4>
<p>If you are running on Linux with an AMD GPU, install the ROCm versions of PyTorch and <code>onnxruntime</code>:</p>
<pre><code class="language-shell">&gt; pip install &quot;torch==1.13.1&quot; &quot;torchvision==0.14.1&quot; --extra-index-url https://download.pytorch.org/whl/rocm5.2
# and one of
&gt; pip install https://download.onnxruntime.ai/onnxruntime_training-1.14.1%2Brocm54-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
# or
&gt; pip install https://download.onnxruntime.ai/onnxruntime_training-1.15.0.dev20230326001%2Brocm542-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
</code></pre>
<p>Make sure you have installed ROCm 5.x (<a href="https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.2.3/page/How_to_Install_ROCm.html#_How_to_Install">see their documentation
</a> for more
details) and that the version of <code>onnxruntime</code> matches your ROCm drivers. The version of PyTorch does not need to match
exactly, and they only have limited versions available.</p>
<p>Ubuntu 20.04 supports ROCm 5.2 and Ubuntu 22.04 supports ROCm 5.4, unless you want to build custom packages. The ROCm
5.x series supports many discrete AMD cards since the Vega 20 architecture, with <a href="https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.4.3/page/Prerequisites.html#d5434e465">a partial list of supported cards
shown here</a>.</p>
<h4 id="for-amd-on-windows-pytorch-cpu-and-onnx-runtime-directml">For AMD on Windows: PyTorch CPU and ONNX runtime DirectML</h4>
<p>If you are running on Windows with an AMD GPU, install the DirectML ONNX runtime as well:</p>
<pre><code class="language-shell">&gt; pip install &quot;torch==1.13.1&quot; &quot;torchvision==0.14.1&quot; --extra-index-url https://download.pytorch.org/whl/cpu
# and one of
&gt; pip install onnxruntime-directml
# or
&gt; pip install ort-nightly-directml --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall
</code></pre>
<p>If you DirectML package upgrades numpy to an incompatible version &gt;= 1.24, downgrade it:</p>
<pre><code class="language-shell">&gt; pip install &quot;numpy&gt;=1.20,&lt;1.24&quot; --force-reinstall  # the DirectML package will upgrade numpy to 1.24, which will not work
</code></pre>
<p>You can optionally install the latest DirectML ORT nightly package, which may provide a substantial performance
increase.</p>
<h4 id="for-cpu-everywhere-pytorch-cpu-and-onnx-runtime-cpu">For CPU everywhere: PyTorch CPU and ONNX runtime CPU</h4>
<p>If you are running with a CPU and no hardware acceleration, install <code>onnxruntime</code> and the CPU version of PyTorch:</p>
<pre><code class="language-shell">&gt; pip install &quot;torch==1.13.1&quot; &quot;torchvision==0.14.1&quot; --extra-index-url https://download.pytorch.org/whl/cpu
# and
&gt; pip install onnxruntime
# or
&gt; pip install ort-nightly --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall
</code></pre>
<h4 id="for-nvidia-everywhere-install-pytorch-gpu-and-onnx-gpu">For Nvidia everywhere: Install PyTorch GPU and ONNX GPU</h4>
<p>If you are running with an Nvidia GPU on any operating system, install <code>onnxruntime-gpu</code> and the CUDA version of
PyTorch:</p>
<pre><code class="language-shell">&gt; pip install &quot;torch==1.13.1&quot; &quot;torchvision==0.14.1&quot; --extra-index-url https://download.pytorch.org/whl/cu117
# and
&gt; pip install onnxruntime-gpu
# or
&gt; pip install ort-nightly-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall
</code></pre>
<p>Make sure you have installed CUDA 11.x and that the version of PyTorch matches the version of CUDA
(<a href="https://pytorch.org/get-started/locally/">see their documentation</a> for more details).</p>
<h3 id="test-the-models">Test the models</h3>
<p>You should verify that all of the steps up to this point have worked correctly by attempting to run the
<code>api/scripts/test-diffusers.py</code> script, which is a slight variation on the original txt2img script.</p>
<p>If the script works, there will be an image of an astronaut in <code>outputs/test.png</code>.</p>
<p>If you get any errors, check <a href="../user-guide/#known-errors">the known errors section of the user guide</a>.</p>
<h3 id="download-the-web-ui-bundle">Download the web UI bundle</h3>
<p>Once the server environment is working, you will need the latest files for the web UI. This is a Javascript bundle and
you can download a pre-built copy from Github or compile your own.</p>
<p>From <a href="https://github.com/ssube/onnx-web/tree/gh-pages">the <code>gh-pages</code> branch</a>, select the version matching your server
and download all three files:</p>
<ul>
<li><code>bundle/main.js</code></li>
<li><code>config.json</code></li>
<li><code>index.html</code></li>
</ul>
<p>Copy them into your local <code>api/gui</code> folder, making sure to keep the <code>main.js</code> bundle in the <code>bundle</code> subfolder.</p>
<p>For example, for a v0.11 server, copy the files from https://github.com/ssube/onnx-web/tree/gh-pages/v0.11.0 into your
local copy of https://github.com/ssube/onnx-web/tree/main/api/gui and
https://github.com/ssube/onnx-web/tree/main/api/gui/bundle.</p>
<h2 id="windows-specific-methods">Windows-specific methods</h2>
<p>These methods are specific to Windows, tested on Windows 10, and still experimental. They should provide an easier
setup experience.</p>
<h3 id="windows-all-in-one-bundle">Windows all-in-one bundle</h3>
<ol>
<li>Install the latest Visual C++ 2019 redistributable<ol>
<li>https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170</li>
<li>https://aka.ms/vs/17/release/vc_redist.x64.exe</li>
</ol>
</li>
<li>Download the latest ZIP file from <a href="https://artifacts.apextoaster.com/#browse/browse:onnx-web-dist">the apextoaster Nexus server</a></li>
<li>Find the ZIP file and <code>Extract All</code> to a memorable folder</li>
<li>Open the folder where you extracted the files<ol>
<li>Your models will be converted into the <code>models</code> folder, and you can <a href="../user-guide/#adding-your-own-models">add your own models</a></li>
<li>Your images will be in the <code>outputs</code> folder, along with files containing the parameters used to generate them</li>
</ol>
</li>
<li>Make sure the server is allowed to run<ol>
<li>Open the <code>server</code> folder</li>
<li>Right-click the <code>onnx-web.exe</code> file and click <code>Properties</code></li>
<li>On the <code>General</code> tab, click <code>Unblock</code> next to the message <code>This file came from another computer and might be
    blocked to help protect this computer.</code></li>
<li>Go back to the folder where you extracted the files</li>
<li>Repeat step 3 for the <code>onnx-web-*.bat</code> files</li>
</ol>
</li>
<li>Run the local server using one of the <code>onnx-web-*.bat</code> scripts<ol>
<li>Run <code>onnx-web-half.bat</code> if you are using a GPU and you have &lt; 12GB of VRAM
    - <code>-half</code> mode is compatible with both AMD and Nvidia GPUs
    - <code>-half</code> mode is not compatible with CPU mode</li>
<li>Run <code>onnx-web-full.bat</code> if you are using CPU mode or if you have &gt;= 16GB of VRAM
    - Try the <code>onnx-web-half.bat</code> script if you encounter out-of-memory errors or generating images is very slow</li>
</ol>
</li>
<li>Wait for the models to be downloaded and converted<ol>
<li>Most models are distributed in PyTorch format and need to be converted into ONNX format</li>
<li>This only happens once for each model and takes a few minutes</li>
</ol>
</li>
<li>Open one of the URLs shown in the logs in your browser<ol>
<li>This will typically be http://127.0.0.1:5000?api=http://127.0.0.1:5000</li>
<li>If you running the server on a different PC and not accessing it from a browser on the same system, use that PC's
    IP address instead of 127.0.0.1</li>
<li>Any modern browser should work, including Chrome, Edge, and Firefox</li>
<li>Mobile browsers also work, but have stricter mixed-content policies</li>
</ol>
</li>
</ol>
<h3 id="windows-python-installer">Windows Python installer</h3>
<ol>
<li>Install the latest Visual C++ 2019 redistributable<ol>
<li>https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170</li>
<li>https://aka.ms/vs/17/release/vc_redist.x64.exe</li>
</ol>
</li>
<li>Install Git<ul>
<li>https://gitforwindows.org/</li>
</ul>
</li>
<li>Install Python 3.10<ul>
<li>https://www.python.org/downloads/</li>
</ul>
</li>
<li>Clone or download the onnx-web repository<ul>
<li><code>git clone https://github.com/ssube/onnx-web.git</code></li>
<li>https://github.com/ssube/onnx-web/archive/refs/heads/main.zip</li>
</ul>
</li>
<li>Open a command prompt window</li>
<li>Run one of the <code>setup-*.bat</code> scripts<ol>
<li>Run <code>setup-amd.bat</code> if you are using an AMD GPU and DirectML</li>
<li>Run <code>setup-nvidia.bat</code> if you are using an Nvidia GPU and CUDA</li>
<li>Run <code>setup-cpu.bat</code> if you are planning on only using CPU mode</li>
</ol>
</li>
<li>After the first run, you can run <code>launch.bat</code> instead of the setup script<ol>
<li>You should only need to run the setup script once</li>
<li>If you encounter any errors with Python imports, run the setup script again</li>
</ol>
</li>
</ol>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../server-admin/" class="btn btn-neutral float-left" title="Server Administration"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../user-guide/" class="btn btn-neutral float-right" title="User Guide">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/ssube/onnx-web" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../server-admin/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../user-guide/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
